{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "from netam import noof, shmoof\n",
    "from epam.torch_common import pick_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = shmoof.load_shmoof_dataframes(\"/Users/matsen/data/shmoof_edges_11-Jan-2023_NoNode0_iqtree_K80+R_masked.csv\", sample_count=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 3608 training examples and 1392 validation examples\n"
     ]
    }
   ],
   "source": [
    "kmer_length = 5\n",
    "max_length = 500\n",
    "\n",
    "train_dataset = shmoof.SHMoofDataset(train_df, kmer_length=kmer_length, max_length=max_length)\n",
    "val_dataset = shmoof.SHMoofDataset(val_df, kmer_length=kmer_length, max_length=max_length)\n",
    "\n",
    "print(f\"we have {len(train_dataset)} training examples and {len(val_dataset)} validation examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training...\n",
      "Epoch [1/10]\t Loss: 0.071663097\t Val Loss: 0.069396995\n",
      "Epoch [2/10]\t Loss: 0.061702187\t Val Loss: 0.069825951\n",
      "Epoch [3/10]\t Loss: 0.061513994\t Val Loss: 0.069570079\n",
      "Epoch [4/10]\t Loss: 0.060912887\t Val Loss: 0.068465089\n",
      "Epoch [5/10]\t Loss: 0.060269527\t Val Loss: 0.06773097\n",
      "Epoch [6/10]\t Loss: 0.059777284\t Val Loss: 0.067315882\n",
      "Epoch [7/10]\t Loss: 0.059450003\t Val Loss: 0.066974686\n",
      "Epoch [8/10]\t Loss: 0.05928794\t Val Loss: 0.066756285\n",
      "Epoch [9/10]\t Loss: 0.05914167\t Val Loss: 0.066699952\n",
      "Epoch [10/10]\t Loss: 0.059067863\t Val Loss: 0.06659212\n"
     ]
    }
   ],
   "source": [
    "model = noof.NoofModel(train_dataset, embedding_dim=2, nhead=2, dim_feedforward=512, layer_count=3, dropout=0.1)\n",
    "\n",
    "# device = pick_device()\n",
    "# train_dataset.to(device)\n",
    "# val_dataset.to(device)\n",
    "# model.to(device)\n",
    "\n",
    "burrito = shmoof.NoofBurrito(train_dataset, val_dataset, model, batch_size=1024, learning_rate=0.5, l2_regularization_coeff=1e-6)\n",
    "print(\"starting training...\")\n",
    "losses = burrito.train(epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "         382,  501,  979,  844,  303,  187,  748,  944,  701,  754,  968,  797,\n",
       "         114,  456,  799,  123,  489,  931,  650,  552,  159,  635,  489,  932,\n",
       "         654,  566,  215,  858,  358,  405,  595,  330,  294,  150,  598,  342,\n",
       "         341,  339,  331,  299,  169,  673,  643,  523,   43,  171,  682,  680,\n",
       "         671,  635,  489,  931,  652,  559,  187,  745,  932,  656,  575,  251,\n",
       "        1003,  939,  681,  673,  641,  516,   14,   53,  209,  836,  270,   53,\n",
       "         212,  845,  307,  204,  815,  187,  745,  929,  643,  522,   37,  146,\n",
       "         582,  277,   81,  322,  264,   29,  114,  453,  785,   66,  262,   22,\n",
       "          87,  348,  366,  438,  726,  856,  350,  373,  465,  835,  265,   35,\n",
       "         140,  558,  183,  729,  867,  396,  558,  181,  722,  838,  277,   84,\n",
       "         333,  308,  206,  821,  211,  844,  301,  179,  713,  802,  133,  530,\n",
       "          71,  284,  110,  438,  725,  849,  323,  265,   33,  130,  518,   21,\n",
       "          83,  332,  304,  190,  760,  990,  886,  470,  856,  351,  377,  481,\n",
       "         899,  522,   40,  159,  633,  483,  906,  552,  158,  632,  479,  892,\n",
       "         495,  953,  738,  902,  535,   90,  358,  407,  602,  359,  411,  617,\n",
       "         418,  645,  530,   71,  283,  106,  424,  671,  636,  495,  956,  749,\n",
       "         948,  720,  829,  242,  968,  799,  124,  495,  954,  743,  921,  611,\n",
       "         393,  547,  139,  554,  166,  663,  604,  368,  446,  759,  985,  868,\n",
       "         397,  561,  196,  784,   61,  244,  976,  829,  244,  975,  825,  228,\n",
       "         909,  561,  196,  781,   51,  204,  815,  187,  748,  944,  704,  768,\n",
       "        1024, 1024, 1021, 1009,  962,  776,   29,  114,  456,  799,  123,  491,\n",
       "         939,  682,  678,  661,  595,  331,  299,  169,  673,  642,  518,   22,\n",
       "          88,  351,  379,  492,  942,  693,  722,  838,  279,   92,  366,  440,\n",
       "         734,  886,  472,  862,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0], dtype=torch.int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/16/pkph6n2962g3lf2wpwwt0wwr0000gn/T/ipykernel_3232/838397735.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  untrained_model(torch.tensor(train_dataset[0:5][0]).to(\"cpu\"))[:,100:105]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0830,  0.1990,  2.3263,  1.3730,  1.6612],\n",
       "        [ 0.9430,  0.1540,  0.2526,  1.3730,  1.6612],\n",
       "        [ 0.0830,  0.3618,  0.1525,  0.5621,  0.9796],\n",
       "        [ 0.0696,  0.9046,  1.4014,  0.2176,  1.6827],\n",
       "        [ 0.9430,  0.6074,  0.2316, 16.9417,  3.7528]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "untrained_model = noof.NoofModel(train_dataset, embedding_dim=2, nhead=2, dim_feedforward=512, layer_count=3, dropout=0.5)\n",
    "\n",
    "\n",
    "#untrained_model(torch.tensor(train_dataset[0][0]).unsqueeze(0).to(\"cpu\"))\n",
    "untrained_model(torch.tensor(train_dataset[0:5][0]).to(\"cpu\"))[:,100:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/16/pkph6n2962g3lf2wpwwt0wwr0000gn/T/ipykernel_3232/3090242575.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model(torch.tensor(train_dataset[0:5][0]))[:,100:105]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.6581, 0.3463, 1.5333, 6.9397, 3.4632],\n",
       "        [0.4631, 0.3333, 2.1145, 6.9397, 3.4632],\n",
       "        [0.6581, 0.3157, 1.7103, 2.6826, 2.5516],\n",
       "        [1.4022, 2.0454, 3.5936, 2.6157, 1.5485],\n",
       "        [0.4631, 0.3508, 2.3401, 5.6431, 1.8637]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor(train_dataset[0:5][0]))[:,100:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"_ignore/noof_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5518, 0.6624, 0.5783, 0.5351, 0.6392, 0.6453, 0.5759, 0.5612, 0.5350,\n",
       "        0.5212, 0.6106, 0.6459, 0.5476, 0.7167, 0.5565, 0.6073, 0.6326, 0.6027,\n",
       "        0.6465, 0.5447, 0.5773, 0.6841, 0.6078, 0.6000, 0.6216, 0.6074, 0.5323,\n",
       "        0.5783, 0.5401, 0.5363, 0.5842, 0.5510, 0.5664, 0.6187, 0.6309, 0.5810,\n",
       "        0.6130, 0.5705, 0.6055, 0.6525, 0.6094, 0.6024, 0.5915, 0.5348, 1.0906,\n",
       "        0.6054, 0.4977, 0.5493, 0.5011, 0.5907, 0.6027, 0.5100, 0.5653, 0.5549,\n",
       "        0.4949, 0.5748, 0.5445, 0.4786, 0.5407, 0.5437, 0.5157, 0.5394, 0.4904,\n",
       "        0.5689, 0.4229, 0.1278, 0.1571, 1.3315, 0.4496, 0.5732, 3.5157, 3.2419,\n",
       "        0.4779, 0.4740, 0.5374, 0.6583, 0.5848, 1.9642, 1.7882, 2.4409, 1.0033,\n",
       "        0.6571, 7.3481, 4.5779, 1.1166, 0.3505, 3.8996, 1.9366, 3.1124, 1.7170,\n",
       "        1.9919, 8.7818, 7.5836, 2.7489, 2.4826, 2.8937, 0.8431, 2.8570, 2.4638,\n",
       "        2.8824, 0.4257, 1.2694, 1.6310, 2.4068, 0.8749, 0.1843, 0.0125, 0.5162,\n",
       "        0.6268, 0.4026, 0.2470, 0.1467, 0.2290, 0.8815, 0.2732, 0.3698, 0.2902,\n",
       "        0.9999, 1.0713, 0.6097, 1.9680, 0.0761, 0.3601, 0.4970, 0.3884, 0.6160,\n",
       "        0.5548, 0.6105, 0.6379, 0.3251, 0.5959, 0.5460, 0.2192, 0.5784, 0.9732,\n",
       "        0.4966, 0.5219, 0.9744, 0.2721, 0.2653, 0.4916, 0.8548, 0.3102, 0.2577,\n",
       "        0.2298, 0.3392, 1.3343, 2.8652, 0.8637, 1.8219, 0.8874, 0.4457, 2.3958,\n",
       "        0.7562, 2.1211, 0.6965, 1.6216, 2.5657, 1.2468, 0.8932, 2.2308, 0.9177,\n",
       "        1.0855, 0.7091, 1.5969, 0.8479, 1.2420, 0.7577, 1.5162, 0.6310, 2.5594,\n",
       "        3.0443, 4.1899, 0.9004, 0.6041, 0.9444, 1.4510, 0.9647, 1.3955, 1.1574,\n",
       "        0.4326, 0.5444, 3.2544, 0.2866, 0.6953, 0.7944, 0.1798, 0.3639, 1.8127,\n",
       "        0.3723, 0.4449, 0.2416, 0.6341, 0.5421, 0.9590, 0.7800, 0.3584, 0.3620,\n",
       "        0.2765, 0.5498, 1.0744, 1.1762, 0.3188, 0.1974, 0.9609, 0.7643, 0.5737,\n",
       "        1.3136, 0.3204, 0.5197, 0.7965, 0.4512, 0.7622, 0.4243, 3.0607, 0.4288,\n",
       "        0.5631, 0.2655, 0.2021, 0.9985, 1.0416, 0.4757, 0.8053, 0.3509, 0.5295,\n",
       "        1.1345, 0.5101, 0.4565, 0.7471, 3.1841, 2.0210, 0.9740, 0.5852, 0.3957,\n",
       "        0.4634, 0.9629, 0.7292, 1.2363, 2.0183, 1.4166, 2.1775, 0.2618, 1.0354,\n",
       "        0.5896, 0.3344, 1.4641, 0.6424, 1.6158, 0.6959, 0.8777, 2.8998, 1.3674,\n",
       "        0.6188, 0.7263, 1.3695, 0.3391, 0.2180, 1.6938, 1.0578, 2.6780, 0.3730,\n",
       "        0.3715, 0.6020, 0.9923, 0.8708, 0.2711, 0.3268, 0.2274, 0.5235, 0.4306,\n",
       "        0.8546, 0.7734, 0.3743, 0.2182, 0.9034, 0.9525, 0.6273, 1.1360, 2.0828,\n",
       "        0.6683, 1.1369, 0.9932, 0.8151, 1.3479, 1.3324, 0.4802, 0.3571, 0.3906,\n",
       "        0.2550, 0.5389, 0.3118, 1.0147, 0.3478, 0.2248, 0.2778, 0.4976, 2.2055,\n",
       "        0.2474, 1.0937, 0.1732, 0.7486, 0.5531, 1.4814, 1.0633, 0.4529, 0.5826,\n",
       "        0.1353, 0.4122, 0.4044, 0.3641, 0.3951, 0.3476, 0.2904, 0.2945, 0.4544,\n",
       "        0.4494, 0.8236, 1.8597, 1.9645, 3.3243, 2.9704, 1.8808, 1.1474, 1.7788,\n",
       "        0.8366, 0.9372, 2.3728, 1.8795, 0.9650, 0.8060, 2.5148, 1.4911, 1.2072,\n",
       "        0.6632, 0.5545, 1.0890, 0.3772, 0.9409, 0.4260, 0.8536, 0.6758, 1.2065,\n",
       "        1.0713, 0.1915, 0.0572, 0.0923, 0.2635, 0.2861, 0.6255, 0.2957, 0.4580,\n",
       "        0.5388, 0.2492, 0.3125, 0.4470, 1.0481, 0.8171, 0.2485, 0.8585, 0.9453,\n",
       "        0.5223, 0.4281, 1.0943, 0.2190, 0.7773, 0.5312, 0.5484, 0.9839, 0.5946,\n",
       "        0.3112, 0.4595, 0.2354, 0.5824, 0.0579, 0.2647, 0.1988, 0.5255, 0.4661,\n",
       "        0.3890, 0.3191, 0.4326, 0.3709, 0.3712, 0.4563, 0.6748, 0.4900, 0.3926,\n",
       "        0.2334, 0.2590, 0.2986, 0.2657, 0.1902, 0.2828, 0.5363, 0.3981, 0.8856,\n",
       "        0.3885, 1.0857, 0.6999, 0.5224, 0.4699, 0.4955, 0.4418, 0.4684, 0.4987,\n",
       "        0.5075, 0.6112, 0.5114, 0.6217, 0.4557, 0.5392, 0.4917, 0.5781, 0.5376,\n",
       "        0.6373, 0.5229, 0.6212, 0.5509, 0.5225, 0.6036, 0.5059, 0.6083, 0.5353,\n",
       "        0.6115, 0.6020, 0.6029, 0.6161, 0.6272, 0.6236, 0.4619, 0.5461, 0.6052,\n",
       "        0.6028, 0.5876, 0.6038, 0.5783, 0.6805, 0.6318, 0.5857, 0.7488, 0.6047,\n",
       "        0.5850, 0.5703, 0.5758, 0.5080, 0.5231, 0.6249, 0.6227, 0.5286, 0.5627,\n",
       "        0.5902, 0.6036, 0.6081, 0.5988, 0.5695, 0.5865, 0.5417, 0.5078, 0.6111,\n",
       "        0.6227, 0.7007, 0.6342, 0.6121, 0.5346, 0.6032, 0.6923, 0.6968, 0.6190,\n",
       "        0.5344, 0.5807, 0.5349, 0.5968, 0.5212, 0.6105, 0.5783, 0.6620, 0.6304,\n",
       "        0.5348, 0.5623, 0.5905, 0.5062, 0.5949, 0.6196, 0.6293, 0.6478, 0.5514,\n",
       "        0.6036, 0.6288, 0.5939, 0.5755, 0.6146, 0.5746, 0.5641, 0.5924, 0.6214,\n",
       "        0.5818, 0.6228, 0.6258, 0.6163, 0.5863], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xmodel = torch.load(\"_ignore/noof_model.pt\")\n",
    "xmodel.eval()\n",
    "\n",
    "xmodel(val_dataset.encoded_parents)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset.masks[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
