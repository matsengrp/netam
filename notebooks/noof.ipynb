{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "from netam import noof, shmoof\n",
    "from epam.torch_common import pick_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00403216242498992"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df = pd.read_csv(\"/Users/matsen/data/shmoof_edges_11-Jan-2023_NoNode0_iqtree_K80+R_masked.csv\")\n",
    "\n",
    "# Here's the fraction of sequences of length more than 410\n",
    "(all_df[\"parent\"].str.len() > 410).sum() / len(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = shmoof.load_shmoof_dataframes(\"/Users/matsen/data/shmoof_edges_11-Jan-2023_NoNode0_iqtree_K80+R_masked.csv\") #, sample_count=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 35830 training examples and 13186 validation examples\n"
     ]
    }
   ],
   "source": [
    "kmer_length = 5\n",
    "max_length = 410\n",
    "\n",
    "train_dataset = shmoof.SHMoofDataset(train_df, kmer_length=kmer_length, max_length=max_length)\n",
    "val_dataset = shmoof.SHMoofDataset(val_df, kmer_length=kmer_length, max_length=max_length)\n",
    "\n",
    "print(f\"we have {len(train_dataset)} training examples and {len(val_dataset)} validation examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Metal Performance Shaders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matsen/mambaforge/envs/epam/lib/python3.9/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training...\n",
      "Epoch [1/100]\t Loss: 0.062672443\t Val Loss: 0.071376044\n",
      "Epoch [2/100]\t Loss: 0.062656802\t Val Loss: 0.071376045\n",
      "Epoch [3/100]\t Loss: 0.062655699\t Val Loss: 0.071376304\n",
      "Epoch [4/100]\t Loss: 0.062656239\t Val Loss: 0.071376042\n",
      "Epoch [5/100]\t Loss: 0.062656593\t Val Loss: 0.071375996\n",
      "Epoch [6/100]\t Loss: 0.062657303\t Val Loss: 0.071376017\n",
      "Epoch 00006: reducing learning rate of group 0 to 2.0000e-02.\n",
      "Epoch [7/100]\t Loss: 0.062657595\t Val Loss: 0.071375995\n",
      "Epoch [8/100]\t Loss: 0.062657396\t Val Loss: 0.071376009\n",
      "Epoch [9/100]\t Loss: 0.062655896\t Val Loss: 0.071375995\n",
      "Epoch [10/100]\t Loss: 0.062655913\t Val Loss: 0.071376145\n",
      "Epoch [11/100]\t Loss: 0.062657752\t Val Loss: 0.071376106\n",
      "Epoch 00011: reducing learning rate of group 0 to 4.0000e-03.\n",
      "Epoch [12/100]\t Loss: 0.062658255\t Val Loss: 0.071376185\n",
      "Epoch [13/100]\t Loss: 0.06265699\t Val Loss: 0.071376001\n",
      "Epoch [14/100]\t Loss: 0.062657448\t Val Loss: 0.071375996\n",
      "Epoch [15/100]\t Loss: 0.062656733\t Val Loss: 0.071376007\n",
      "Epoch [16/100]\t Loss: 0.062656792\t Val Loss: 0.071375998\n",
      "Epoch 00016: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch [17/100]\t Loss: 0.062658028\t Val Loss: 0.071376085\n",
      "Epoch [18/100]\t Loss: 0.062658547\t Val Loss: 0.071376063\n",
      "Epoch [19/100]\t Loss: 0.062657557\t Val Loss: 0.071376045\n",
      "Epoch [20/100]\t Loss: 0.062654017\t Val Loss: 0.071376114\n",
      "Epoch [21/100]\t Loss: 0.062658908\t Val Loss: 0.071376045\n",
      "Epoch 00021: reducing learning rate of group 0 to 1.6000e-04.\n",
      "Epoch [22/100]\t Loss: 0.062656456\t Val Loss: 0.071376055\n",
      "Epoch [23/100]\t Loss: 0.062655302\t Val Loss: 0.071376058\n",
      "Epoch [24/100]\t Loss: 0.062657289\t Val Loss: 0.071376058\n",
      "Epoch [25/100]\t Loss: 0.062657422\t Val Loss: 0.07137606\n",
      "Epoch [26/100]\t Loss: 0.062655549\t Val Loss: 0.07137606\n",
      "Epoch 00026: reducing learning rate of group 0 to 3.2000e-05.\n",
      "Epoch [27/100]\t Loss: 0.062656789\t Val Loss: 0.07137606\n",
      "Epoch [28/100]\t Loss: 0.062656204\t Val Loss: 0.07137606\n",
      "Epoch [29/100]\t Loss: 0.062655286\t Val Loss: 0.071376059\n",
      "Epoch [30/100]\t Loss: 0.062656843\t Val Loss: 0.071376058\n",
      "Epoch [31/100]\t Loss: 0.062656456\t Val Loss: 0.071376058\n",
      "Epoch 00031: reducing learning rate of group 0 to 6.4000e-06.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/matsen/re/netam/notebooks/noof.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/matsen/re/netam/notebooks/noof.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m burrito \u001b[39m=\u001b[39m shmoof\u001b[39m.\u001b[39mNoofBurrito(train_dataset, val_dataset, model, batch_size\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m, learning_rate\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, l2_regularization_coeff\u001b[39m=\u001b[39m\u001b[39m1e-6\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/matsen/re/netam/notebooks/noof.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mstarting training...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/matsen/re/netam/notebooks/noof.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m losses \u001b[39m=\u001b[39m burrito\u001b[39m.\u001b[39;49mtrain(epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "File \u001b[0;32m~/re/netam/netam/shmoof.py:200\u001b[0m, in \u001b[0;36mNoofBurrito.train\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m    198\u001b[0m training_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m    199\u001b[0m \u001b[39mfor\u001b[39;00m encoded_parents, masks, mutation_indicators \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_loader:\n\u001b[0;32m--> 200\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_calculate_loss(encoded_parents, masks, mutation_indicators)\n\u001b[1;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    203\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/re/netam/netam/shmoof.py:245\u001b[0m, in \u001b[0;36mNoofBurrito._calculate_loss\u001b[0;34m(self, encoded_parents, masks, mutation_indicators)\u001b[0m\n\u001b[1;32m    241\u001b[0m mutation_freq \u001b[39m=\u001b[39m mutation_indicators\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m/\u001b[39m masks\u001b[39m.\u001b[39msum(\n\u001b[1;32m    242\u001b[0m     dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    243\u001b[0m )\n\u001b[1;32m    244\u001b[0m mut_prob \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m torch\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39mrates \u001b[39m*\u001b[39m mutation_freq)\n\u001b[0;32m--> 245\u001b[0m mut_prob_masked \u001b[39m=\u001b[39m mut_prob[masks]\n\u001b[1;32m    246\u001b[0m mutation_indicator_masked \u001b[39m=\u001b[39m mutation_indicators[masks]\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m    247\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(mut_prob_masked, mutation_indicator_masked)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = noof.NoofModel(train_dataset, embedding_dim=2, nhead=1, dim_feedforward=512, layer_count=3, dropout=0.1)\n",
    "\n",
    "device = pick_device()\n",
    "train_dataset.to(device)\n",
    "val_dataset.to(device)\n",
    "model.to(device)\n",
    "\n",
    "burrito = shmoof.NoofBurrito(train_dataset, val_dataset, model, batch_size=1024, learning_rate=0.1, l2_regularization_coeff=1e-6)\n",
    "print(\"starting training...\")\n",
    "losses = burrito.train(epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"_ignore/noof_model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
