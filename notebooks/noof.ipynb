{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "from netam import noof, shmoof\n",
    "from epam.torch_common import pick_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_id\n",
       "326651    22424\n",
       "326713    13186\n",
       "327059     4686\n",
       "316188     3067\n",
       "326797     3028\n",
       "326780     1573\n",
       "326737      442\n",
       "326650      324\n",
       "326907      286\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_shmoof_df = pd.read_csv(\"/Users/matsen/data/shmoof_edges_11-Jan-2023_NoNode0_iqtree_K80+R_masked.csv\", index_col=0).reset_index(drop=True)\n",
    "\n",
    "# only keep rows where parent is different than child\n",
    "full_shmoof_df = full_shmoof_df[full_shmoof_df[\"parent\"] != full_shmoof_df[\"child\"]]\n",
    "\n",
    "# full_shmoof_df = full_shmoof_df.sample(5000)\n",
    "\n",
    "# Hold out sample 326713\n",
    "val_df = full_shmoof_df[full_shmoof_df[\"sample_id\"] == 326713]\n",
    "train_df = full_shmoof_df.drop(val_df.index)\n",
    "\n",
    "full_shmoof_df.sample_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 35830 training examples and 13186 validation examples\n"
     ]
    }
   ],
   "source": [
    "kmer_length = 5\n",
    "max_length = 500\n",
    "\n",
    "train_dataset = shmoof.SHMoofDataset(train_df, kmer_length=kmer_length, max_length=max_length)\n",
    "val_dataset = shmoof.SHMoofDataset(val_df, kmer_length=kmer_length, max_length=max_length)\n",
    "\n",
    "print(f\"we have {len(train_dataset)} training examples and {len(val_dataset)} validation examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Metal Performance Shaders\n",
      "starting training...\n",
      "Epoch [1/10]\t Loss: 17.779089\t Val Loss: 20.12041\n",
      "Epoch [2/10]\t Loss: 17.776786\t Val Loss: 20.119459\n",
      "Epoch [3/10]\t Loss: 17.776683\t Val Loss: 20.119612\n",
      "Epoch [4/10]\t Loss: 17.776736\t Val Loss: 20.119473\n",
      "Epoch [5/10]\t Loss: 17.776671\t Val Loss: 20.119497\n",
      "Epoch [6/10]\t Loss: 17.776854\t Val Loss: 20.119454\n",
      "Epoch [7/10]\t Loss: 17.776714\t Val Loss: 20.119643\n",
      "Epoch [8/10]\t Loss: 17.776706\t Val Loss: 20.119541\n",
      "Epoch [9/10]\t Loss: 17.776675\t Val Loss: 20.120012\n",
      "Epoch [10/10]\t Loss: 17.776774\t Val Loss: 20.11947\n"
     ]
    }
   ],
   "source": [
    "model = noof.NoofModel(train_dataset, embedding_dim=2, nhead=2, dim_feedforward=512, layer_count=3, dropout=0.1)\n",
    "\n",
    "device = pick_device()\n",
    "train_dataset.to(device)\n",
    "val_dataset.to(device)\n",
    "model.to(device)\n",
    "\n",
    "burrito = noof.NoofBurrito(train_dataset, val_dataset, model, learning_rate=0.1, l2_regularization_coeff=1e-6)\n",
    "print(\"starting training...\")\n",
    "losses = burrito.train(epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,  415,  636,  494,  952,  734,\n",
       "         888,  479,  891,  492,  944,  701,  756,  976,  830,  246,  981,  852,\n",
       "         334,  309,  211,  842,  293,  147,  588,  303,  187,  748,  944,  701,\n",
       "         754,  968,  800,  126,  504,  991,  891,  491,  938,  678,  664,  607,\n",
       "         379,  489,  932,  654,  566,  215,  859,  362,  421,  659,  586,  294,\n",
       "         150,  598,  342,  341,  339,  331,  299,  169,  673,  643,  523,   43,\n",
       "         169,  674,  648,  543,  123,  489,  931,  652,  559,  187,  745,  932,\n",
       "         656,  575,  251, 1003,  937,  675,  652,  557,  180,  718,  824,  222,\n",
       "         888,  478,  885,  468,  845,  307,  204,  815,  187,  747,  937,  675,\n",
       "         650,  549,  147,  586,  296,  157,  626,  456,  798,  118,  469,  849,\n",
       "         323,  268,   46,  183,  732,  878,  438,  726,  856,  350,  373,  465,\n",
       "         835,  265,   35,  140,  558,  183,  729,  867,  396,  558,  181,  724,\n",
       "         846,  309,  212,  846,  312,  222,  885,  467,  844,  301,  179,  713,\n",
       "         802,  133,  530,   71,  284,  110,  438,  725,  849,  322,  261,   17,\n",
       "          66,  262,   21,   82,  328,  288,  126,  504,  990,  886,  470,  856,\n",
       "         351,  377,  481,  899,  524,   48,  191,  761,  994,  902,  536,   94,\n",
       "         376,  479,  892,  495,  953,  738,  902,  535,   90,  358,  407,  602,\n",
       "         357,  403,  585,  290,  133,  530,   71,  283,  106,  422,  661,  594,\n",
       "         325,  276,   77,  308,  208,  829,  242,  968,  799,  124,  495,  954,\n",
       "         743,  921,  611,  393,  547,  137,  546,  135,  539,  107,  428,  685,\n",
       "         689,  708,  784,   61,  244,  975,  825,  226,  903,  539,  108,  432,\n",
       "         702,  757,  980,  848,  320,  255, 1017,  995,  908,  557,  178,  712,\n",
       "         799,  123,  491,  939,  682,  678,  661,  595,  331,  299,  169,  673,\n",
       "         644,  526,   54,  215,  857,  355,  396,  558,  183,  730,  870,  407,\n",
       "         604,  366,  440,  734,  886,  472,  862,  373,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0], device='mps:0',\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/16/pkph6n2962g3lf2wpwwt0wwr0000gn/T/ipykernel_67957/838397735.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  untrained_model(torch.tensor(train_dataset[0:5][0]).to(\"cpu\"))[:,100:105]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0246, 1.0246, 0.9760, 1.0246, 1.0246],\n",
       "        [1.0246, 1.0246, 0.9760, 0.9760, 1.0246],\n",
       "        [1.0246, 0.9760, 0.9760, 1.0246, 0.9760],\n",
       "        [1.0246, 0.9760, 0.9760, 0.9760, 0.9760],\n",
       "        [0.9760, 0.9760, 0.9760, 1.0246, 1.0246]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "untrained_model = noof.NoofModel(train_dataset, embedding_dim=2, nhead=2, dim_feedforward=512, layer_count=3, dropout=0.5)\n",
    "\n",
    "\n",
    "#untrained_model(torch.tensor(train_dataset[0][0]).unsqueeze(0).to(\"cpu\"))\n",
    "untrained_model(torch.tensor(train_dataset[0:5][0]).to(\"cpu\"))[:,100:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/16/pkph6n2962g3lf2wpwwt0wwr0000gn/T/ipykernel_67957/3090242575.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model(torch.tensor(train_dataset[0:5][0]))[:,100:105]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0236, 1.0236, 1.0236, 1.0236, 1.0236],\n",
       "        [1.0236, 1.0236, 1.0236, 1.0236, 1.0236],\n",
       "        [1.0236, 1.0236, 1.0236, 1.0236, 1.0236],\n",
       "        [1.0236, 1.0236, 1.0236, 1.0236, 1.0236],\n",
       "        [1.0236, 1.0236, 1.0236, 1.0236, 1.0236]], device='mps:0',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor(train_dataset[0:5][0]))[:,100:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"_ignore/noof_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (13186) must match the size of tensor b (5000) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/matsen/re/netam/notebooks/noof.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/matsen/re/netam/notebooks/noof.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m xmodel \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39m_ignore/noof_model.pt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/matsen/re/netam/notebooks/noof.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m xmodel\u001b[39m.\u001b[39meval()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/matsen/re/netam/notebooks/noof.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m xmodel(val_dataset\u001b[39m.\u001b[39;49mencoded_parents)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/mambaforge/envs/epam/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/epam/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/re/netam/netam/noof.py:57\u001b[0m, in \u001b[0;36mNoofModel.forward\u001b[0;34m(self, encoded_parents)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# log_rates = self.kmer_embedding(encoded_parents).squeeze()\u001b[39;00m\n\u001b[1;32m     56\u001b[0m kmer_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkmer_embedding(encoded_parents)\n\u001b[0;32m---> 57\u001b[0m kmer_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos_encoder(kmer_embeddings)\n\u001b[1;32m     59\u001b[0m \u001b[39m# Pass through the transformer encoder\u001b[39;00m\n\u001b[1;32m     60\u001b[0m transformer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(kmer_embeddings)\n",
      "File \u001b[0;32m~/mambaforge/envs/epam/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/epam/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/re/epam/epam/torch_common.py:96\u001b[0m, in \u001b[0;36mPositionalEncoding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m     92\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[39m    Arguments:\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[39m        x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpe[: x\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39;49m)]\n\u001b[1;32m     97\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (13186) must match the size of tensor b (5000) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "xmodel = torch.load(\"_ignore/noof_model.pt\")\n",
    "xmodel.eval()\n",
    "\n",
    "xmodel(val_dataset.encoded_parents)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset.masks[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
