{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from netam import framework, models\n",
    "from netam.framework import calculate_loss\n",
    "from epam.torch_common import pick_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00403216242498992"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shmoof_data_path = \"/Users/matsen/data/shmoof_edges_11-Jan-2023_NoNode0_iqtree_K80+R_masked.csv\"\n",
    "all_df = pd.read_csv(shmoof_data_path)\n",
    "\n",
    "# Here's the fraction of sequences of length more than 410\n",
    "(all_df[\"parent\"].str.len() > 410).sum() / len(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = framework.load_shmoof_dataframes(\"/Users/matsen/data/shmoof_edges_11-Jan-2023_NoNode0_iqtree_K80+R_masked.csv\") #, sample_count=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Metal Performance Shaders\n",
      "we have 35830 training examples and 13186 validation examples\n"
     ]
    }
   ],
   "source": [
    "kmer_length = 3\n",
    "max_length = 410\n",
    "\n",
    "train_dataset = framework.SHMoofDataset(train_df, kmer_length=kmer_length, max_length=max_length)\n",
    "val_dataset = framework.SHMoofDataset(val_df, kmer_length=kmer_length, max_length=max_length)\n",
    "\n",
    "device = pick_device()\n",
    "train_dataset.to(device)\n",
    "val_dataset.to(device)\n",
    "\n",
    "print(f\"we have {len(train_dataset)} training examples and {len(val_dataset)} validation examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNModel(\n",
       "  (kmer_embedding): Embedding(65, 12)\n",
       "  (conv): Conv1d(12, 13, kernel_size=(7,), stride=(1,), padding=same)\n",
       "  (linear): Linear(in_features=13, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, dataset, embedding_dim, num_filters, kernel_size):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.kmer_count = len(dataset.kmer_to_index)\n",
    "\n",
    "        self.kmer_embedding = nn.Embedding(self.kmer_count, embedding_dim)\n",
    "\n",
    "        # Convolutional layer\n",
    "        self.conv = nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=kernel_size, padding='same')\n",
    "\n",
    "        self.linear = nn.Linear(in_features=num_filters, out_features=1)\n",
    "\n",
    "    def forward(self, encoded_parents, masks):\n",
    "        kmer_embeds = self.kmer_embedding(encoded_parents)\n",
    "        # Need to do transpose because conv1D expects the channel dimension\n",
    "        # (embedding_dim) to be before the along-sequence dimension. \n",
    "        kmer_embeds = kmer_embeds.permute(0, 2, 1)\n",
    "\n",
    "        # Apply convolutional layer\n",
    "        conv_out = F.relu(self.conv(kmer_embeds))\n",
    "        conv_out = conv_out.permute(0, 2, 1)\n",
    "\n",
    "        # Apply linear layer\n",
    "        log_rates = self.linear(conv_out).squeeze(-1)\n",
    "\n",
    "        # Exponentiate to get rates\n",
    "        rates = torch.exp(log_rates * masks)\n",
    "\n",
    "        return rates\n",
    "\n",
    "\n",
    "model = CNNModel(train_dataset, embedding_dim=12, num_filters=13, kernel_size=7)\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for k-mer length of 3\n",
    "\n",
    "[I 2023-11-18 11:48:04,286] Trial 13 finished with value: 0.06653115217839932 and parameters: {'embedding_dim': 12, 'num_filters': 11, 'kernel_size': 7}. Best is trial 13 with value: 0.06653115217839932.\n",
    "[I 2023-11-18 12:15:52,747] Trial 21 finished with value: 0.06652927552032738 and parameters: {'embedding_dim': 11, 'num_filters': 13, 'kernel_size': 7}. Best is trial 21 with value: 0.06652927552032738.\n",
    "[I 2023-11-18 12:37:04,155] Trial 27 finished with value: 0.06651306199354845 and parameters: {'embedding_dim': 17, 'num_filters': 11, 'kernel_size': 7}. Best is trial 27 with value: 0.06651306199354845.\n",
    "\n",
    "for this model with kmer length = 3\n",
    "model = CNNModel(train_dataset, embedding_dim=12, num_filters=13, kernel_size=7)\n",
    "Epoch [43/50]\t Loss: 0.05789036\t Val Loss: 0.066382327\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for k-mer length of 5\n",
    "\n",
    "[I 2023-11-18 13:05:16,096] Trial 0 finished with value: 0.06628789302387134 and parameters: {'embedding_dim': 10, 'num_filters': 17, 'kernel_size': 7}. Best is trial 0 with value: 0.06628789302387134.\n",
    "....\n",
    "[I 2023-11-18 14:35:12,791] Trial 24 finished with value: 0.06636707468562074 and parameters: {'embedding_dim': 12, 'num_filters': 15, 'kernel_size': 7}. Best is trial 0 with value: 0.06628789302387134."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training...\n",
      "Epoch [0/50]\t Loss: 0.058427606\t Val Loss: 0.066611466\n",
      "Epoch [1/50]\t Loss: 0.058895127\t Val Loss: 0.066749167\n",
      "Epoch [2/50]\t Loss: 0.058507647\t Val Loss: 0.066771644\n",
      "Epoch [3/50]\t Loss: 0.058480443\t Val Loss: 0.066700835\n",
      "Epoch [4/50]\t Loss: 0.058442706\t Val Loss: 0.066774304\n",
      "Epoch [5/50]\t Loss: 0.058428341\t Val Loss: 0.066680854\n",
      "Epoch [6/50]\t Loss: 0.058429926\t Val Loss: 0.066661189\n",
      "Epoch [7/50]\t Loss: 0.058385966\t Val Loss: 0.066658454\n",
      "Epoch [8/50]\t Loss: 0.058428884\t Val Loss: 0.066707124\n",
      "Epoch [9/50]\t Loss: 0.05840453\t Val Loss: 0.066634798\n",
      "Epoch [10/50]\t Loss: 0.058391592\t Val Loss: 0.066730997\n",
      "Epoch [11/50]\t Loss: 0.058412626\t Val Loss: 0.066734078\n",
      "Epoch [12/50]\t Loss: 0.058366708\t Val Loss: 0.066653705\n",
      "Epoch [13/50]\t Loss: 0.058376967\t Val Loss: 0.066677899\n",
      "Epoch [14/50]\t Loss: 0.058376679\t Val Loss: 0.066660937\n",
      "Epoch 00014: reducing learning rate of group 0 to 2.0000e-02.\n",
      "Epoch [15/50]\t Loss: 0.058126266\t Val Loss: 0.06644326\n",
      "Epoch [16/50]\t Loss: 0.058001318\t Val Loss: 0.066413834\n",
      "Epoch [17/50]\t Loss: 0.057986705\t Val Loss: 0.066423074\n",
      "Epoch [18/50]\t Loss: 0.057979684\t Val Loss: 0.066463167\n",
      "Epoch [19/50]\t Loss: 0.057962943\t Val Loss: 0.066418026\n",
      "Epoch [20/50]\t Loss: 0.057979012\t Val Loss: 0.066472058\n",
      "Epoch [21/50]\t Loss: 0.05796575\t Val Loss: 0.066467535\n",
      "Epoch 00021: reducing learning rate of group 0 to 4.0000e-03.\n",
      "Epoch [22/50]\t Loss: 0.057873788\t Val Loss: 0.066386877\n",
      "Epoch [23/50]\t Loss: 0.057840491\t Val Loss: 0.066392806\n",
      "Epoch [24/50]\t Loss: 0.057833443\t Val Loss: 0.066403297\n",
      "Epoch [25/50]\t Loss: 0.05782892\t Val Loss: 0.066397812\n",
      "Epoch [26/50]\t Loss: 0.057824987\t Val Loss: 0.066406218\n",
      "Epoch [27/50]\t Loss: 0.057826207\t Val Loss: 0.066411534\n",
      "Epoch 00027: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch [28/50]\t Loss: 0.057799969\t Val Loss: 0.066401298\n",
      "Epoch [29/50]\t Loss: 0.057793232\t Val Loss: 0.066400262\n",
      "Epoch [30/50]\t Loss: 0.057790484\t Val Loss: 0.066403678\n",
      "Epoch [31/50]\t Loss: 0.057791304\t Val Loss: 0.066401413\n",
      "Epoch [32/50]\t Loss: 0.057787484\t Val Loss: 0.066404381\n",
      "Epoch 00032: reducing learning rate of group 0 to 1.6000e-04.\n",
      "Epoch [33/50]\t Loss: 0.057781421\t Val Loss: 0.066405014\n",
      "Epoch [34/50]\t Loss: 0.057782397\t Val Loss: 0.066404559\n",
      "Epoch [35/50]\t Loss: 0.057785095\t Val Loss: 0.066404803\n",
      "Epoch [36/50]\t Loss: 0.057782296\t Val Loss: 0.066404175\n",
      "Epoch [37/50]\t Loss: 0.057781843\t Val Loss: 0.066405521\n",
      "Epoch 00037: reducing learning rate of group 0 to 3.2000e-05.\n",
      "Stopping training early: learning rate below 0.0001\n"
     ]
    }
   ],
   "source": [
    "burrito = framework.Burrito(train_dataset, val_dataset, model, batch_size=1024, learning_rate=0.1, min_learning_rate=1e-5, l2_regularization_coeff=1e-6)\n",
    "print(\"starting training...\")\n",
    "losses = burrito.train(epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 65)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset.all_kmers), len(our_val_dataset.all_kmers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on 51...\n",
      "evaluating on small...\n",
      "evaluating on 13...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nickname</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51</td>\n",
       "      <td>0.059682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>small</td>\n",
       "      <td>0.054704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>0.066406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  nickname      loss\n",
       "0       51  0.059682\n",
       "1    small  0.054704\n",
       "2       13  0.066406"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nicknames = [\"51\", \"small\", \"13\"]\n",
    "val_losses = []\n",
    "for nickname in nicknames:\n",
    "    our_train_df, our_val_df = framework.load_shmoof_dataframes(shmoof_data_path, val_nickname=nickname)\n",
    "    our_val_dataset = framework.SHMoofDataset(our_val_df, kmer_length=kmer_length, max_length=max_length)\n",
    "    our_val_dataset.to(device)\n",
    "    our_val_loader = torch.utils.data.DataLoader(our_val_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "    print(f\"evaluating on {nickname}...\")\n",
    "    # our_burrito = framework.Burrito(our_train_dataset, our_val_dataset, model, batch_size=1024, learning_rate=0.1, min_learning_rate=1e-4, l2_regularization_coeff=1e-6)\n",
    "    val_losses.append(burrito.process_data_loader(our_val_loader, train_mode=False))\n",
    "\n",
    "val_df = pd.DataFrame({\"nickname\": nicknames, \"loss\": val_losses})\n",
    "val_df\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.save(model, \"_ignore/cnn_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matsen/mambaforge/envs/epam/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2023-11-18 13:01:45,504] A new study created in memory with name: no-name-01189937-bf16-4618-8e1a-7c37a9505fa0\n",
      "[I 2023-11-18 13:05:16,096] Trial 0 finished with value: 0.06628789302387134 and parameters: {'embedding_dim': 10, 'num_filters': 17, 'kernel_size': 7}. Best is trial 0 with value: 0.06628789302387134.\n",
      "[I 2023-11-18 13:08:34,205] Trial 1 finished with value: 0.06644169138918223 and parameters: {'embedding_dim': 8, 'num_filters': 6, 'kernel_size': 5}. Best is trial 0 with value: 0.06628789302387134.\n",
      "[I 2023-11-18 13:12:28,156] Trial 2 finished with value: 0.06641458698723397 and parameters: {'embedding_dim': 15, 'num_filters': 19, 'kernel_size': 5}. Best is trial 0 with value: 0.06628789302387134.\n",
      "[I 2023-11-18 13:16:23,008] Trial 3 finished with value: 0.0663018548626938 and parameters: {'embedding_dim': 16, 'num_filters': 14, 'kernel_size': 7}. Best is trial 0 with value: 0.06628789302387134.\n",
      "[I 2023-11-18 13:19:48,446] Trial 4 finished with value: 0.0665061096356379 and parameters: {'embedding_dim': 10, 'num_filters': 17, 'kernel_size': 3}. Best is trial 0 with value: 0.06628789302387134.\n",
      "[I 2023-11-18 13:23:27,860] Trial 5 finished with value: 0.06631091468733431 and parameters: {'embedding_dim': 12, 'num_filters': 8, 'kernel_size': 7}. Best is trial 0 with value: 0.06628789302387134.\n",
      "[I 2023-11-18 13:27:04,765] Trial 6 finished with value: 0.06645340774867706 and parameters: {'embedding_dim': 17, 'num_filters': 12, 'kernel_size': 3}. Best is trial 0 with value: 0.06628789302387134.\n",
      "[I 2023-11-18 13:30:41,119] Trial 7 finished with value: 0.06648539597512955 and parameters: {'embedding_dim': 17, 'num_filters': 5, 'kernel_size': 3}. Best is trial 0 with value: 0.06628789302387134.\n",
      "[I 2023-11-18 13:34:24,769] Trial 8 finished with value: 0.06642247998744281 and parameters: {'embedding_dim': 8, 'num_filters': 9, 'kernel_size': 5}. Best is trial 0 with value: 0.06628789302387134.\n",
      "[I 2023-11-18 13:38:23,467] Trial 9 finished with value: 0.06637022681799858 and parameters: {'embedding_dim': 19, 'num_filters': 19, 'kernel_size': 5}. Best is trial 0 with value: 0.06628789302387134.\n",
      "[I 2023-11-18 13:42:13,485] Trial 10 finished with value: 0.06639476963507465 and parameters: {'embedding_dim': 6, 'num_filters': 15, 'kernel_size': 7}. Best is trial 0 with value: 0.06628789302387134.\n",
      "[I 2023-11-18 13:46:02,167] Trial 11 finished with value: 0.06641290310968602 and parameters: {'embedding_dim': 13, 'num_filters': 14, 'kernel_size': 7}. Best is trial 0 with value: 0.06628789302387134.\n",
      "[I 2023-11-18 13:49:55,786] Trial 12 finished with value: 0.06634554100469665 and parameters: {'embedding_dim': 13, 'num_filters': 11, 'kernel_size': 7}. Best is trial 0 with value: 0.06628789302387134.\n",
      "[I 2023-11-18 13:53:43,017] Trial 13 finished with value: 0.06643713433021445 and parameters: {'embedding_dim': 11, 'num_filters': 15, 'kernel_size': 7}. Best is trial 0 with value: 0.06628789302387134.\n",
      "[I 2023-11-18 13:57:47,617] Trial 14 finished with value: 0.06637377916396921 and parameters: {'embedding_dim': 15, 'num_filters': 17, 'kernel_size': 7}. Best is trial 0 with value: 0.06628789302387134.\n",
      "[I 2023-11-18 14:01:44,087] Trial 15 finished with value: 0.06638228005041541 and parameters: {'embedding_dim': 20, 'num_filters': 17, 'kernel_size': 7}. Best is trial 0 with value: 0.06628789302387134.\n",
      "[I 2023-11-18 14:05:27,425] Trial 16 finished with value: 0.06641753963435634 and parameters: {'embedding_dim': 5, 'num_filters': 20, 'kernel_size': 7}. Best is trial 0 with value: 0.06628789302387134.\n",
      "[I 2023-11-18 14:09:17,264] Trial 17 finished with value: 0.06634425507268214 and parameters: {'embedding_dim': 15, 'num_filters': 13, 'kernel_size': 7}. Best is trial 0 with value: 0.06628789302387134.\n",
      "[I 2023-11-18 14:13:00,330] Trial 18 finished with value: 0.06642379952221995 and parameters: {'embedding_dim': 10, 'num_filters': 10, 'kernel_size': 7}. Best is trial 0 with value: 0.06628789302387134.\n",
      "[I 2023-11-18 14:16:30,112] Trial 19 finished with value: 0.06651123842637238 and parameters: {'embedding_dim': 17, 'num_filters': 16, 'kernel_size': 3}. Best is trial 0 with value: 0.06628789302387134.\n",
      "[I 2023-11-18 14:20:07,470] Trial 20 finished with value: 0.06639115119557197 and parameters: {'embedding_dim': 8, 'num_filters': 13, 'kernel_size': 7}. Best is trial 0 with value: 0.06628789302387134.\n",
      "[I 2023-11-18 14:23:52,505] Trial 21 finished with value: 0.06630358462804332 and parameters: {'embedding_dim': 11, 'num_filters': 7, 'kernel_size': 7}. Best is trial 0 with value: 0.06628789302387134.\n",
      "[I 2023-11-18 14:27:32,429] Trial 22 finished with value: 0.06632149524859046 and parameters: {'embedding_dim': 10, 'num_filters': 7, 'kernel_size': 7}. Best is trial 0 with value: 0.06628789302387134.\n",
      "[I 2023-11-18 14:31:20,634] Trial 23 finished with value: 0.06633492224014494 and parameters: {'embedding_dim': 14, 'num_filters': 11, 'kernel_size': 7}. Best is trial 0 with value: 0.06628789302387134.\n",
      "[I 2023-11-18 14:35:12,791] Trial 24 finished with value: 0.06636707468562074 and parameters: {'embedding_dim': 12, 'num_filters': 15, 'kernel_size': 7}. Best is trial 0 with value: 0.06628789302387134.\n",
      "[W 2023-11-18 14:37:27,593] Trial 25 failed with parameters: {'embedding_dim': 7, 'num_filters': 18, 'kernel_size': 7} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matsen/mambaforge/envs/epam/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/16/pkph6n2962g3lf2wpwwt0wwr0000gn/T/ipykernel_35607/578153883.py\", line 44, in objective\n",
      "    for encoded_parents, masks, mutation_indicators in val_loader:\n",
      "  File \"/Users/matsen/mambaforge/envs/epam/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/Users/matsen/mambaforge/envs/epam/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 674, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/Users/matsen/mambaforge/envs/epam/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/Users/matsen/mambaforge/envs/epam/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/Users/matsen/re/netam/netam/framework.py\", line 78, in __getitem__\n",
      "    return self.encoded_parents[idx], self.masks[idx], self.mutation_indicators[idx]\n",
      "KeyboardInterrupt\n",
      "[W 2023-11-18 14:37:27,595] Trial 25 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/matsen/re/netam/notebooks/cnn.ipynb Cell 12\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matsen/re/netam/notebooks/cnn.ipynb#X10sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m best_val_loss\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matsen/re/netam/notebooks/cnn.ipynb#X10sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/matsen/re/netam/notebooks/cnn.ipynb#X10sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matsen/re/netam/notebooks/cnn.ipynb#X10sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39m# Print the results\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matsen/re/netam/notebooks/cnn.ipynb#X10sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest trial:\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/epam/lib/python3.9/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     _optimize(\n\u001b[1;32m    452\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    453\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    454\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    455\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    456\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    457\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    458\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    459\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    460\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    461\u001b[0m     )\n",
      "File \u001b[0;32m~/mambaforge/envs/epam/lib/python3.9/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/mambaforge/envs/epam/lib/python3.9/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/mambaforge/envs/epam/lib/python3.9/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/mambaforge/envs/epam/lib/python3.9/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "\u001b[1;32m/Users/matsen/re/netam/notebooks/cnn.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matsen/re/netam/notebooks/cnn.ipynb#X10sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m val_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matsen/re/netam/notebooks/cnn.ipynb#X10sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/matsen/re/netam/notebooks/cnn.ipynb#X10sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     \u001b[39mfor\u001b[39;00m encoded_parents, masks, mutation_indicators \u001b[39min\u001b[39;00m val_loader:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matsen/re/netam/notebooks/cnn.ipynb#X10sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m         rates \u001b[39m=\u001b[39m model(encoded_parents, masks)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matsen/re/netam/notebooks/cnn.ipynb#X10sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m         loss \u001b[39m=\u001b[39m calculate_loss(rates, masks, mutation_indicators)\n",
      "File \u001b[0;32m~/mambaforge/envs/epam/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/mambaforge/envs/epam/lib/python3.9/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/mambaforge/envs/epam/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/mambaforge/envs/epam/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/re/netam/netam/framework.py:78\u001b[0m, in \u001b[0;36mSHMoofDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[0;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoded_parents[idx], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmasks[idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmutation_indicators[idx]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 1024\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters to optimize\n",
    "    embedding_dim = trial.suggest_int(\"embedding_dim\", 5, 20)\n",
    "    num_filters = trial.suggest_int(\"num_filters\", 5, 20)\n",
    "    kernel_size = trial.suggest_categorical(\"kernel_size\", [3, 5, 7])\n",
    "\n",
    "    # Initialize the model with the suggested hyperparameters\n",
    "    model = CNNModel(train_dataset, embedding_dim=embedding_dim, num_filters=num_filters, kernel_size=kernel_size)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Define optimizer, loss criterion, etc.\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # You might need to adjust this depending on your train and validation loop implementation\n",
    "    epochs = 40  # Number of epochs to train the model\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for encoded_parents, masks, mutation_indicators in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            rates = model(encoded_parents, masks)\n",
    "            loss = calculate_loss(rates, masks, mutation_indicators)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for encoded_parents, masks, mutation_indicators in val_loader:\n",
    "                rates = model(encoded_parents, masks)\n",
    "                loss = calculate_loss(rates, masks, mutation_indicators)\n",
    "                val_loss += loss.item() * encoded_parents.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print the results\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value: {trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
